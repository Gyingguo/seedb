%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}
\label{sec:introduction}

This thesis presents the design and implementation of a system, \SeeDB,
for automatically generating a large number of {\it interesting visualizations}
for any given query. There are two key challenges in automatically generating
visualizations: 1) automatically determining ``interesting''-ness of a
visualization, and 2) evaluating a large number of possible visualizations
efficiently.
In this work, we present solutions to both these problems.

\section{The data analysis process}
Data analysts must sift through very large volumes of data 
to identify trends, insights, or anomalies. 
Given the scale of data, and the relative ease and 
intuitiveness of examining data visually,
analysts often use visualizations as a tool to identify
these trends, insights, and anomalies.
However, selecting the ``right'' visualization often 
remains a laborious and time-consuming task. We illustrate the data analysis
process and challenges in choosing good visualizations using a few examples.

\subsection{Example 1: Business Intelligence}
Consider a dataset containing sales records for a nation-wide
chain of stores.
Let's say the store's data analyst is interested
in examining how the newly-introduced heating device, the ``Laserwave
Oven'', has been doing over the past year.
The results of this analysis will inform business decisions
for the chain, including marketing strategies, and the introduction of a similar
``Saberwave Oven''.

The analysis workflow proceeds as follows:
(1) The analyst poses a query to select the subset of data that she is
interested in exploring.
For instance, for the example above, she may issue the query:
\noindent $${\tt \ Q \ \ = \ \ SELECT \ * \ FROM \ \  Sales \ \ WHERE \ \
Product=``Laserwave''} $$ \noindent Notice that the results for this query may
have (say) several million records each with several dozen attributes.
Thus, directly perusing the query result is simply infeasible.
(2) Next, the analyst studies various properties of the selected data by
constructing diverse views or visualizations from the data. In this particular
scenario, the analyst may want to study total sales by store, quantity in stock
by region, or average profits by month. To construct these views, the analyst
can use operations such as binning, grouping, and aggregation, and then generate
visualizations from the view. For example, to generate the view `total sales by
store', the analyst would group each sales record based on the store where the
sale took place and sum up the sale amounts per store. This operation can easily
be expressed as the familiar aggregation over group-by query:
\noindent
\begin{align*}
& \tt Q' = SELECT \ \ store,\ SUM(amount) \ \ FROM \ \  Sales \ \ WHERE \\
& \tt Product=``Laserwave" \ \ GROUP  \ \ BY \ \ store
\end{align*}
The result of the above query is a two-column table that can then be visualized
as a bar-chart. Table \ref{tab:staplerX} and Figure
\ref{fig:staplerX} respectively show an example of the results of this view and
the associated visualization.

\begin{figure}[htb]

  \centering
    \includegraphics[width=12cm]{Images/dist1.pdf}
  \caption{Data: Total Sales by Store for Laserwave}
  \label{fig:staplerX}
\end{figure}

\begin{table}[htb]
  \centering
  \begin{tabular}{cc} \hline
  Store & Total Sales (\$) \\ \hline
  Cambridge, MA & 180.55 \\ \hline
  Seattle, WA &  145.50\\ \hline
  New York, NY & 122.00 \\ \hline
  San Francisco, CA & 90.13 \\ \hline
  \end{tabular}
  \caption{Data: Total Sales by Store for Laserwave}\label{tab:staplerX} 
\end{table}

\begin{figure}[htb]
  \centering
    \includegraphics[width=12cm]{Images/dist2.pdf}
\caption{Scenario A: Total Sales by Store}
\label{fig:staplerX-a}
\end{figure}

\begin{figure}[htb]
  \centering
    \includegraphics[width=12cm]{Images/dist3.pdf}
  \caption{Scenario B: Total Sales by Store}
  \label{fig:staplerX-b}
\end{figure}

To explore the query results from different perspectives, the analyst generates
a large number of views (and visualizations) of the form described above.
(3) The analyst then manually examines each view and decides
which ones are ``interesting''. This is a critical and time-consuming step.
Naturally, what makes a view interesting depends on the 
application semantics and the trend we are comparing against.
For instance, the view of Laserwave sales by store, as shown in Figure
\ref{fig:staplerX}, may be interesting if the overall sales of all products show
the {\it opposite} trend (e.g. Figure \ref{fig:staplerX-a}). However, the same
view may be uninteresting if the sales of all products follow a similar trend (Figure \ref{fig:staplerX-b}).
Thus, we posit that  a view is {\em potentially ``interesting'' if it shows 
a trend in the subset of data selected by the analyst
(i.e., Laserwave product-related data)
that deviates from the equivalent trend in the overall dataset}.
Of course, the analyst must decide if this deviation 
is truly an insight for this application.
(4) Once the analyst has identified interesting views, the analyst may
then either share these views with others, further interact with
the displayed views (e.g., by drilling down or rolling up), or
start afresh with a new query.


Of the four steps in the workflow described above, the 
ones that are especially repetitive and tedious are steps (2) and (3),
where the analyst generates a large number of candidate views, and examines each
of them in turn. The goal of our system, \SeeDB, is to automate these
labor-intensive steps of the workflow. Given a query $Q$ indicating the subset
of data that the analyst is interested in, \SeeDB\ automatically {\em identifies and highlights to the analyst the most
interesting views of the query results using methods based on
deviation}. Specifically, \SeeDB\ explores the space of all possible views and
measures how much each view deviates from the corresponding view on the
entire underlying dataset (e.g. Figure~\ref{fig:staplerX} vs.
Figures~\ref{fig:staplerX-a} or \ref{fig:staplerX-b}.) By generating and
scoring potential views automatically, \SeeDB\ effectively eliminates
steps (2) and (3) that the analyst currently performs. Instead, once \SeeDB\
recommends interesting views, the analyst can evaluate this small
subset of views using domain knowledge and limit further
exploration to these views.  

\subsection{Example 2: Medical Data}
Next, let us examine a use case in a completely different problem domain that
also involves analyses similar to Example 1, and therefore, can benefit from the
automatic construction of interesting views of a user query.

Consider a medical researcher studying the cost of care for cancer
patients\footnote{Real use scenario at an area hospital}. The goal of the study
is to identify patients whose cost of care is significantly greater than
average and try to explain the high cost of care. Potential reasons for
high cost include treatments for late-stage cancers, old age of the
population, longer survival time (chemotherapy for a longer duration), type of
treatment etc.
Note that the goal is {\it not} to build a predictive model for cost; rather, it
is to perform exploratory analysis to explain {\it why} certain patients have
high cost of care.
As a first pass, assume that the researcher identifies high cost patients as those with
cost that is greater than two standard deviations away from the average
cost. This can be expressed via the following SQL query, assuming a table of
patients with their treatment costs and other treatment-related information.
\begin{align*}
& \tt Q2\ = SELECT \ \ *\ FROM \ \ Patients \ \ where\ cost - \\
  \ \ & \tt (SELECT\ AVG(cost)\ FROM\ Patients)\ >\\
  \ \ & \tt 2\ *\ (SELECT\ STDDEV(cost)\ FROM\ Patients)
\end{align*}

Once these patients have been identified, the researcher can begin to analyze
the data to find potential reasons for their high cost of care (This is similar
to Step 1 in Example 1).
One technique to find potential reasons for high cost is to compare the
high-cost population of patients to the remaining set of patients (called
``low-cost'' population).
Similar to Example 1 above, we posit that the {\it characteristics that
explain high cost are precisely those characteristics that are different between
the high-cost and low-cost population}. 
For instance, if the majority of
high-cost patients were those with late-stage disease (sicker patients) while
the low-cost patients were not, the researcher could reason that the sicker
patients needed more medications or procedures, leading
to higher cost overall. 
Note that this analysis is similar to Example 1 where we
compared the ``total sales by year'' for $Laserwave$ $oven$ vs. all store
products.
In this setup, we want to compare the ``total patients by disease stage'' for
high-cost patients vs. low-cost patients. The only change in the problem
formulation is that we are now comparing views of query Q2 against views of the
remaining table, instead of views over the entire table. The rest of the
framework remains unchanged. Therefore, \SeeDB\ can be used to automatically
construct a large number of views of the high-cost population, compare
each view to the corresponding view over the low-cost population, and
identify views showing the highest difference between the two populations. These
views identify potential causes for high cost. Once the researcher has identified views of interest, he or she
can follow up with more complex analyses like statistical significance testing or machine
learning.

\subsection{Example 3: Product Analysis}

Consider a company like Facebook\footnote{www.facebook.com} that continuously
deploys changes to its website and mobile apps, and tracks user interaction
through detailed logging. Product specialists at Facebook use these logs to
study how different Facebook users respond to changes to the web or mobile
experience \cite{DBLP:conf/vldb/AbrahamABB13}. Each user can be characterized by a large number
of features such as location, age, device used, number of friends etc.
For each user, the logs note the actions taken on the website or app such
as likes, shares, comments, page visits etc. For example, in order to study
the user response to an update to the mobile app, the specialist compares user
interaction metrics for a large number of mobile users before and after the
update (usually a week on week comparison). The goal is to find patterns in the
interaction metrics based on different user characteristics. Therefore, if it
appears that the average number of app visits on the iOS app have reduced
signficantly following the update, it would indicate a problem with the iOS app.

In terms of the \SeeDB\ framework, log data from the week following the app
update constitutes the query results we seek to analyze and the log data from
the prior week comprises the comparison dataset. As in Examples 1 and 2, the
analysis process requires the specialist to create diverse views of the query
results, compare views to equivalent views on the comparison dataset, and
pick the views showing the highest difference. 
\SeeDB\ can therefore be used to automate this process and surface only the most
interesting views.

\subsection{\SeeDB}
This thesis describes a prototype of the \SeeDB\ system that finds interesting
visualizations of query results. 
Given a query $Q$ indicating the subset of data that the analyst is interested in, \SeeDB\ automatically {\em identifies and highlights to the
analyst the most interesting views of the query results using methods based on
deviation}. 
Our prototype is based on
\cite{DBLP:conf/vldb/Parameswaran2013} which describes the vision for \SeeDB.

To efficiently and accurately identify interesting views of any given query, we
are faced with the following challenges:
\begin{itemize}
  \item We must determine metrics that accurately measure the ``deviation'' of a
view with respect to the equivalent view on the entire database (e.g.,
Figure~\ref{fig:staplerX} vs.~\ref{fig:staplerX-a}), while simultaneouly
ensuring that \SeeDB\ is not tied to any particular metric(s)
\item We must
intelligently explore the space of candidate views. Since the number of
candidate views (or visualizations) increases as the square of the number of
attributes in a table (we will demonstrate this in subsequent sections),
generating and evaluating all views, even for a moderately sized dataset (e.g.
1M rows, 100 attributes), can be prohibitively expensive
\item While executing
queries corresponding to different views, we must share computation as much as
possible. For example, we can compute multiple views and measure their deviation
all together in one query. Independent execution, on the other hand, will be
expensive and wasteful
\item Since analysis must happen in real-time, we must trade-off accuracy of
visualizations or estimation of ``interestingness'' for reduced latency.
\end{itemize}

Contributions of the work described in this thesis are:
\begin{itemize}
  \item We implement the \SeeDB\ system based on \cite{DBLP:conf/vldb/Parameswaran2013} to
  automatically find interesting views of queries (Chapter
  \ref{sec:system_architecture}).
  \item We propose and implement a number of optimizations to
  efficiently perform the search for interesting views and share
  computation between multiple views simultaneously (Chapter
  \ref{sec:optimizations}).
  \item We evaluate the performance of our optimizations on a range of datasets
  and demonstrate the resulting 100x speedup (Chapter
  \ref{sec:experiments}).
  \item We model the performance of \SeeDB\ in terms of various parameters of
  \SeeDB\ and the underlying database, and use this model to identify optimal
  parameter settings for \SeeDB\ (Chapter \ref{sec:model}).
  %\item We present and evaluate different metrics for measuring deviation
  %\item ?? We study the usefulness of a tool like \SeeDB\ in finding
  %interesting visualizations of a dataset
\end{itemize}



% of Step 1), we demonstrate that we can automatically explore various views of
% that data, evaluate each one for ``interesting''-ness and only surface the most
% promising views to the analyst. 



% In this demo, we demonstrate a system called \SeeDB\
% \cite{DBLP:conf/vldb/Parameswaran2013} that automates the labor-intensive parts
% of the aforementioned data analysis process by automatically identifying and
% producing high-quality views for any input query. Specifically, given a query
% $Q$ posed by the user, \SeeDB\ explores all possible views of $Q$, determines
% the ``interesting''-ness of each of the views based on deviation and returns to
% the user the set of views that it deems the most interesting. The user can then
% limit his analysis to this high-quality set of views.

% In the process of automatically producing an interesting set of views for any
% query, \SeeDB\ must address a few challenges: (a) the size of the space of
% potential views increases as the square of the number of attributes in a table,
% and even for a moderately sized table (e.g. 1M rows, 100 attributes) generating
% all views is prohibitively expensive; as a result, \SeeDB\ must intelligently
% explore this space; (b) computing each view and its utility independently is
% expensive and wasteful, and hence \SeeDB\ must share computation between
% queries; and (c) since visual analysis must happen in real-time, \SeeDB\ must
% tradeoff accuracy of views for reduced latency. In Section
% \ref{sec:system_architecture}, we describe how \SeeDB\ addresses these
% challenges.


% Consider a medical researcher studying the cost of care for cancer patients. Her
% research involves the analysis of a set of 1M electronic medical records (EMRs).
% To analyze this data, the researcher identifies patients that cost
% significantly more than the average: specifically, she selects patients whose
% cost of care is greater than the average cost by two standard deviations. In
% terms of SQL, she runs the following query: \\

% \noindent 
% \begin{small}
% \begin{verbatim}
% Q = SELECT * FROM Patients where total_cost - 
% (SELECT AVG(total_cost) from Patients) as avg_cost
% > 2 * (SELECT STDDEV(total_cost) from Patients);
% \end{verbatim}
% \end{small}

% Once she has identified these patients, she must study various aspects of their
% care to determine the reason why the patients have large cost of care. For
% instance, she may study length of treatment, survival rate, severity of disease
% etc. For each of these parameters, she is interested in determining how the
% group of patients with high cost of care are different from the overall group of
% patients. As a result, she may construct various views of the data that
% compare various metrics between the high cost patients and the overall patient
% population. For instance, she may compare the distribution of length of
% treatment for the two populations, the average severity of the disease
% etc. Since there are a large number of metrics that may be responsible for high
% cost of care, the analyst must construct, visualize and examine a large number
% of views to identify interesting trends. For more than 5 metrics, this process
% quickly becomes tedious and time-consuming. We can significantly simplify and
% speed up the analysis process if we can automate the creation and evaluation of
% views.


% \begin{figure}
% \vspace{-10pt}
% \CenterFloatBoxes
% \begin{floatrow}
% \ttabbox{%
%   \small
%   \begin{tabular}{cc} \hline
%   Store & Total Sales (\$) \\ \hline
%   Cambridge, MA & 180.55 \\ \hline
%   Seattle, WA &  145.50\\ \hline
%   New York, NY & 122.00 \\ \hline
%   San Francisco, CA & 90.13 \\ \hline
%   \end{tabular}
% }{%
%   \caption{Data: Total Sales by Store for Laserwave}\label{tab:staplerX}%
% }
% \killfloatstyle
% \ffigbox{%
%   \hbox{\resizebox{2cm}{2cm}{\includegraphics{Images/dist1.pdf}}}
%   
% }{%
%   \caption{Visualization: Total \\ Sales by Store for
%   Laserwave}
%   \label{fig:staplerX}%
% }
% \end{floatrow}
% \end{figure} 
%  
% \begin{figure}
% \CenterFloatBoxes
% \begin{floatrow}
% \centering
% \ffigbox{%
%   \hbox{\resizebox{2cm}{2cm}{\includegraphics{Images/dist2.pdf}}}
% 
% }{ \caption{Scenario A: Total Sales by Store}\label{fig:staplerX-a}%
% }
% \killfloatstyle
% \ffigbox{%
%   \hbox{\resizebox{2cm}{2cm}{\includegraphics{Images/dist3.pdf}}} 
% 
%   }{\caption{Scenario B: Total Sales by Store}
%   \label{fig:staplerX-b}%
%   }
% \end{floatrow}
% \vspace{-12pt}
% \end{figure}


% Since \SeeDB\ must rank views based on utility, accurately measuring 
% utility is cruicial. \SeeDB\ is based on the principle
% that it is the {\bf deviations from expected behavior that make a view
% interesting}. For instance, in the above example, the researcher would be
% interested in the fact that high-cost patients actually visit a specific set of
% doctors compared to the entire patient population. Similarly, the researcher
% would be interested in knowing that the high-cost patients have longer hospital
% stays compared to the rest of the population. Thus, given a query, interesting
% trends are those that differ significantly between the query and the underlying
% dataset. \SeeDB\ therefore assigns higher utility to views that show divergent
% trends. (Since it may be more appropriate to compare the high-cost patients with
% other patients having the same disease but lower cost, so \SeeDB\ allows the
% user to specify what dataset to compare with).


% \noindent There are several technical challenges that need to be addressed:
% 
% \begin{denselist}
% 
% \item For a given query, $n$, the total number of discriminating views, (even if
% we restrict ourselves to views that append a group-by and an aggregation) is
% likely to be very large to explore exhaustively and precisely. Generating each
% of $R_1(Q(D)),$  $\ldots,$ $R_n(Q(D))$, scoring them on utility, and then
% picking the best one is certainly not feasible for most databases. Thus, we need
% mechanisms to prune the space of views and compute their utility approximately.
% 
% \item Generating and scoring the discriminating views $R_i(Q(D))$ one-by-one may
% miss interesting optimization opportunities: First, we may share computation
% between discriminating views.  For example, the results of two views with
% different aggregates but the same group-by may be computed together in one
% query, followed by projecting out to reveal the two individual views.  Second,
% by evaluating the discriminating views in a deliberate order, we may be able to
% prune views with low utility (without evaluation) that are definitely not going
% to be recommended to the analyst.
% 
% \item Since visualizations tend to convey approximate information, e.g., a trend
% in a line plot may be more important than knowing the exact coordinates of each
% point, we can introduce approximations as part of \SeeDB.  Thus, the utility of
% a discriminating view may be computed approximately but efficiently, and the
% recommended discriminating views can be populated with approximate results,
% based on synopses of the base data or of the query result, that can be generated
% much more efficiently.
% 
% \end{denselist}
