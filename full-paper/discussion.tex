%!TEX root=document.tex


\section{Discussion}
\label{sec:discussion}
We now describe how \SeeDB\ can be extended to even more general settings,
and also propose some directions for future research.

\subsection{Extensions to Other Settings}\label{sec:discussion:ext}

\subsubsection{Generalized Visualizations}\label{sec:discussion:multi-col}
\agp{multi-column viz}

Group-by clauses with multiple dimension attributes: It is
straightforward to extend the \SeeDB\ techniques  to group-by clauses with multiple attributes.
However, for ease of exposition and visualization, we limit the number of
attributes in the group-by clause to one.

\subsubsection{Alternative Problem Definitions}\label{sec:discussion:def}
Instead of comparing the views of
the input query to the entire underlying dataset, it may be more appropriate to
compare them to another subset of the data (e.g. sales of ``Staplers'' vs.
sales of ``Printers''). This simply involves replacing the dataset parameter $D$
with a second query $Q'$. This variation is important since it can help users
find interesting differences in data. Our techniques apply to this variation
unchanged and we show experimental results for this variation.

\subsubsection{Handling attributes of different kinds}
\mpv{Numerical, Temporal, Categorical, Gepgraphical}\\

\subsubsection{Binning}



\subsection{Making SeeDB more efficient} 
There are several ways to extend SeeDB to be more efficient, more
flexible and more helpful in the data analysis process. We now describe some
directions for future work.

\subsubsection {Sampling}
  For large datasets, sampling can be used to significantly improve
  performance. To use sampling with \SeeDB, we precompute a sample of the
  entire dataset (size of sample depends on desired accuracy). When a query is
  issued to \SeeDB, we run all view queries against the sample and pick the
  top-k views. Only these high-utility views are then computed on the entire
  dataset. As expected, the accuracy of views depends on the size of the sample;
  a larger sample generally produces more accurate results and we can develop
  bounds on the accuracy of aggregates computed on samples.
  There are two ways to employ sampling in the \SeeDB\ setting:
  (1) depending on the response time required, choose a sample size that will
  provide the required response time and accordingly return to the user the
  estimated accuracy of the results; or (2) given a user-specific threshold for
  accuracy, determine the correct size of the sample and apply the above
  technique. 
  

  \subsubsection{Deeper Integration}

Another approach to making \SeeDB\ more efficient is to choose a backend DBMS
that is particularly suited for the workloads generated by \SeeDB.
The advantage of having \SeeDB\ as a wrapper over
the database is that we can replace backends without changes to the SeeDB code.
In particular, it would be instructive to compare how databases with different
data layouts can speed up \SeeDB\ processing. One may expect that column stores
like Vertica may be more efficient at processing \SeeDB\ workloads since
individual columns would be stored separately. It is however also likely that
optimization strategies would be significantly different depending on
the data layout.
Similarly, a comparison of column stores vs. main memory databases like VoltDB
could also provide interesting insights.
Finally, we can attempt to speed up workloads like \SeeDB\ by implementing
operators inside the database that can leverage shared scans for tables. 

\subsubsection{Improving Usability} 
For data analysis tools to be effective,
they must achieve the right balance of automation and interactivity in the analysis
process.
For instance, in \SeeDB, merely providing the analyst the system's pick of ten
most interesting views is insufficient. We must not only provide explanations
for our choice but also allow the user to interrogate our views directly and
further manipulate the data iteratively. 

In the future, we could also attempt to learn a distance metric based on user's
feedback. Similarly, we can leverage user feedback to learn the type of views
that a user finds interesting and use that model to prune uninteresting views.


