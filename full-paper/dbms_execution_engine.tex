\section{DBMS-backed Execution Engine}
In this section, we discuss the design and implementation of the \SeeDB\
execution engine backed by a DBMS.
The reason this implementation is appealing is that it enables \SeeDB\ to be
used with a variety of existing database systems with only a thin wrapper around
the DBMS.

\subsection{Basic Framework}
\label{sec:basic_framework}
Given a set of view stubs provided to the execution engine, the basic framework
works as follows: (1) for each view, we generate SQL queries for the target and
comparison view, (2) we executing each view query independently on the DBMS, (3)
the results of each query are processed and normalized to compute the target and
comparison distributions, (4) we compute utility for each view and select
the top-$k$ views with the largest utility.
If the underlying table has $d$ dimension attributes and $m$ measure attributes,
$2\ast d \ast m$ queries must be separately executed and their results
processed. Even for modest size tables (1M tuples, $d$=50, $m$=5), this
technique takes prohibitively long (700s on Postgres). The basic approach is
clearly inefficient since it examines every possible view and executes each view
query independently. We next discuss how our optimizations fix these problems. 

\subsection{Optimizations} 
\label{sec:optimizations}
Since view queries tend to be very similar in structure (they differ in the
aggregation attribute, grouping attribute or subset of data queried), \SeeDB\
uses multiple techniques to intelligently combine view queries. 
In addition, \SeeDB\ leverages
parallelism and partitioning to further reduce query execution time. The
ultimate goal of these optimizations is to minimize scans of the underlying
dataset by sharing as many table scans as possible. 
\SeeDB\ supports the following optimizations and their combinations.

\subsubsection{Combine target and comparison view query}
\label{subsec:target_comparison_view}
Since the target view and comparison views only differ in the subset of data
that the query is executed on, we can easily rewrite these two view queries as
one. For instance, for the target and comparison view queries $Q1$ and $Q2$
shown below, we can add a group by clause to combine the two queries into $Q3$.
\begin{align*} 
Q1 = &{\tt SELECT \ } a, f(m) \ \ {\tt FROM} \  D\  {\tt WHERE \ \ x\ <\ 10\
GROUP \ \ BY} \ a \\
Q2 = &{\tt SELECT \ } a, f(m) \ \ {\tt FROM} \  D\  {\tt GROUP \ \ BY} \ a \\
Q3 = &{\tt SELECT \ } a, f(m), {\tt CASE\ IF\ x\ <\ 10\ THEN\ 1\ ELSE\ 0\
END}\\ 
&as\ group1,\ 1\ as\ group2\\ 
&{\tt FROM} \ D\ {\tt GROUP \ \ BY} \ a,\ group1,\ group2
\end{align*}
This rewriting allows us to obtain results for both queries in a single table
scan. The impact of this optimization depends on the selectivity of the
input query and the presence of indexes. When the input query is less selective,
the query executor must do more work if the two queries are run separately. In
contrast, in the presence of an index, running selective queries independently
may be faster.

\subsubsection {Combine Multiple Aggregates} 
A large number of view queries have the same group-by attribute but different
aggregation attributes. 
%In addition, the majority of real-world datasets,
%tables have few measure attributes but a large number of dimension attributes
(e.g. the Super Store dataset has 5 measure attributes but tens of dimension
%attributes). 
Therefore, \SeeDB\ combines all view queries with the same
group-by attribute into a single, combined view query. For instance, instead of executing
queries for views $(a_1$, $m_1$, $f_1)$, $(a_1$, $m_2$, $f_2)$ \ldots $(a_1$, $m_k$, $f_k)$
independently, we can combine the $n$ views into a single view represented by
$(a_1, \{m_1, m_2\ldots m_k\}$, $\{f_1, f_2\ldots f_k\})$. We demonstrate this
optimization offers a speed-up roughly linear in the number of measure
attributes.

\subsubsection {Combine Multiple Group-bys}
\label{subsec:mult_gb}
  Since \SeeDB\ computes a large number of group-bys, one significant
  optimization is to combine queries with different group-by attributes into a
  single query with multiple group-bys attributes.
  For instance, instead of executing queries for views $(a_1$, $m_1$, $f_1)$,
  $(a_2$, $m_1$, $f_1)$ \ldots $(a_n$, $m_1$, $f_1)$ independently, we can
  combine the $n$ views into a single view represented by $(\{a_1, a_2\ldots
  a_n\}$, $m_1$, $f_1)$ and post-process results at the backend. Alternatively,
  if the SQL GROUPING SETS\footnote{GROUPING SETS allow the simultaneous
  grouping of query results by multiple sets of attributes.} functionality is
  available in the underlying DBMS, \SeeDB\ can leverage that as well.

  While this optimization has the potential to significantly reduce query
  execution time, the number of views that can be combined depends on the
  number of distinct groups present for the given combination of grouping
  attributes. 
  For a large number of distinct groups, the query executor must
  keep track of a large number of aggregates. 
  This increases computational time
  as well as temporary storage requirements, making this technique ineffective.
  The number of distinct groups in turn depends on the correlation between
  values of attributes that are being grouped together. 
  For instance, if two
  dimension attributes $a_i$ and $a_j$ have $n_i$ and $n_j$ distinct values
  respectively and a correlation coefficient of $c$, the number of distinct
  groups when grouping by both $a_i$ and $a_j$ can be approximated by
  $n_i$$\ast$$n_j$$\ast$(1-$c$) for $c$$\neq$1 and $n_i$ for $c$=1 ($n_i$ must
  be equal to $n_j$ in this case).
  As a result, we must combine group-by attributes such that the number of
  distinct groups remains small enough. 
  In Section \ref{sec:experiments}, we
  characterize the performance of this optimization and devise strategies to
  choose dimension attributes that can be grouped together.
  
  
  \mpv{Talk about Huffman encoding based grouping}
  
%   (We discuss our model and
%   algorithm in our full paper~\ref{}).\agp{if full paper is not available
%   by the time of the demo, we can't cite it unfortunately..}
%   A variation of this approach also implemented
%   on \SeeDB\ is to send the results of the multiple group-by query to the front
%   end and ask the \SeeDB\ frontend to compute utility and select views. The
%   advantage of this approach is that it allows for more efficient interactive
%   exploration of the views.

  \subsubsection {Parallel Query Execution}
  \label{subsec:parallel_exec}
  While the above optimizations reduce the number of queries executed, we can
  further speedup \SeeDB\ processing by executing view queries in parallel. When
  executing queries in parallel, we expect co-executing queries to share pages in the
  buffer pool for scans of the same table, thus reducing disk accesses and
  therefore the total execution time. 
  However, a large number of parallel queries can lead to poor performance for
  several reasons including buffer pool contention, locking and cache line
  contention \cite{Postgres_wiki}. 
  As a result, we must identify the optimal number of parallel queries for our workload.
  
  % We do observe a reduction in the
  %overall latency when a small number of queries are executing in parallel;
  % however, the advantages disappear for larger number of queries running in
  % parallel. We discuss this further in the evaluation subsection.

 \subsubsection {Pre-computing Comparison Views}
  We notice that in the case where our comparison view is constructed from the
  entire underlying table (Example 1 in Chapter \ref{sec:introduction}),
  comparison views are the same irrespective of the input query.
  In this case, we can precompute all possible comparison views once and store
  them for use in all future comparisons. If the dataset has $d$ dimension and
  $m$ measure attributes, pre-computing comparison views would add $d$$\ast$$m$
  tables. This corresponds to an extra storage of $O(d\ast m \ast n)$ where $n$
  is the maximum number of distinct values in any of the $d$ attributes. In this
  case, we still need to evaluate each target view, and we can leverage previous
  optimizations to speed up target view generation.
  
  Note that pre-computation cannot be used in situations where the comparison
  view depends on the target view (Example 2) or is directly specified by the
  user (Example 3).
  
\subsubsection {Sampling}
  For large datasets, sampling can be used to significantly improve
  performance. To use sampling with \SeeDB, we precompute a sample of the
  entire dataset (size of sample depends on desired accuracy). When a query is
  issued to \SeeDB, we run all view queries against the sample and pick the
  top-k views. Only these high-utility views are then computed on the entire
  dataset. As expected, the accuracy of views depends on the size of the sample;
  a larger sample generally produces more accurate results and we can develop
  bounds on the accuracy of aggregates computed on samples.

\subsection{Row Stores vs. Column Stores}
\label{sec:row_store_column_store}
The DBMS-backed execution engine for \SeeDB\ is agnostic towards whether the
DBMS is a row store or a column store. However, we expect that some of the above
optimizations (e.g. combining a large number of aggregates) will be more
powerful in row stores and may actually hurt performance in column stores. In
our experimental evaluation, we study the relative advantages of each of our
optimizations.

We now discuss the in-memory implementation of the execution engine.
