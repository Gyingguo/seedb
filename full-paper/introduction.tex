\section{Introduction}
\label{sec:intro}
Data scientists analyze large amounts of data to find interesting trends and
gain novel insights. Given the large scale of data and relative ease of visual
analysis, analysts often use visualizations as a means to identify properties of
interest. The general analysis workflow is usually ad-hoc and can be represented
as follows: (1) The analyst selects a subset of the data they are interested in;
for instance, in a medical use-case (Section \ref{subsec:motivation_example1}),
the analysts may select patients that had expenses more than twice the standard
deviation of all patients. (2) Next, the analyst processes the data in various
ways, e.g. via binning, aggregates, group-bys etc. to generate a large number of
``views'' of the data and builds visualizations such as scatterplots,
histograms, pie charts etc. for each view of the data. (3) The analyst then
studies all the views and identifies the {\it interesting} ones. (4) Once he/she
has identified the interesting views, he/she then drills down into these views
or further explores the properties that show unusual trends. 

Of the four steps in the above process, only two (Steps 1 and 4) require
creative thinking and analyst input. Steps 2 and 3 are tedious and
time-consuming and ideal candidates for automation. Specifically, given the data that the analyst
is interested in (result of Step 1), we can automatically generate various
views of that data, evaluate each one for ``interesting''-ness and only surface
the most promising views. \SeeDB\ automates these labor-intensive parts of data
analysis.

\subsection{Motivating Example I}
\label{subsec:motivation_example1}

Consider a medical researcher studying the cost of care for cancer patients. Her
research involves the analysis of a set of 1M electronic medical records (EMRs).
To analyze this data, the researcher identifies patients that cost
significantly more than the average: specifically, she selects patients whose
cost of care is greater than the average cost by two standard deviations. In
terms of SQL, she runs the following query: \\

\noindent 
\begin{small}
\begin{verbatim}
Q = SELECT * FROM Patients where total_cost - 
(SELECT AVG(total_cost) from Patients) as avg_cost
> 2 * (SELECT STDDEV(total_cost) from Patients);
\end{verbatim}
\end{small}

Once she has identified these patients, she would like to study various aspects
of their care to determine the reason why the patients have large cost of care. For
instance, she may study length of treatment, survival rate, severity of disease
etc. For each of these parameters, she is interested in determining how the
group of patients with high cost of care are different from the overall group of
patients. As a result, she may construct various views of the data that
compare various metrics between the high cost patients and the overall patient
population. For instance, she may compare the distribution of length of
treatment for the two populations, the average severity of the disease
etc. Since there are a large number of metrics that may be responsible for high
cost of care, the analyst must construct, visualize and examine a large number
of views to identify interesting trends. For more than 5 metrics, this process
quickly becomes tedious and time-consuming. We can significantly simplify and
speed up the analysis process if we can automate the creation and evaluation of
views.

\subsection{Motivating Example II}
\label{subsec:motivation_example2}

Consider a store owner who wants to figure out why a certain category of
products (say staplers), is not selling well. 

{\bf Dataset:} For this example, we assume that we are working with the
canonical SuperStore dataset~\cite{superstore} commonly used in
Tableau~\cite{tableau}. This dataset has all sales data for a store selling 50
categories of products over XXX years. Stored as a star-schema, the dataset has
separate tables for Geography, Client information, Shipping details,
Dates etc. connected to the main Sales table via foreign-key constraints. We
show a subset of the schema in Table~\ref{}.

Suppose the store owner runs the following SQL query to select the relevant
stapler sale records.
\noindent 
\begin{small}
\begin{verbatim}
Q = SELECT * FROM sales join products where 
products.name = "Staplers" and sales.year = 2013
\end{verbatim}
\end{small}

To understand why staplers sold poorly in 2013, the owner may want to compare
how staplers did compared to all the products in the store (or another type of
office supply product). Since the number of records is too large to view
individually, the owner will aggregate sales along various dimensions such as
month, geography, customer type etc. and compare the distribution of stapler
sales to overall sales distribution. Any significant differences in two
distributions can point to potential reasons why stapler are going poorly (e.g.
no staplers were shipped to Argentina which is otherwise a major purchaser of
office supplies). We will use this example and the associated dataset as
our running example in the paper.


\subsection{Summary}
In this paper, we describe our system called \SeeDB\
\cite{DBLP:conf/vldb/Parameswaran2013} that partially automates the data
analysis process. Given a query posed by the user, \SeeDB\ can compute a large
number of views based on that query, determine the ``interesting''-ness or
utility of each of the view and show to the user only those views that it deems
most interesting. The researcher can then focus only on the important trends in
the data. \SeeDB\ is based on the principle that it is the {\bf deviations from
expected behavior that make a view interesting}. For instance, in the above
example, the researcher would be interested in the fact that high-cost patients
actually visit a specific set of doctors compared to the entire patient
population. Distribution of doctors across patients would not be interesting if
the distribution was similar for the high cost patients and the overall
population. \SeeDB\ therefore assigns higher utility to views that show
divergent trends in the query and the underlying dataset.

In the process of automatically producing an interesting set of views for any
query, \SeeDB\ must address a few challenges: (a) the size of
the space of potential views increases exponentially with the number of
attributes in a table, as a result, \SeeDB\ must intelligently explore this
space; (b) computing each view and its utility independently is expensive and
wasteful, and hence \SeeDB\ must share computation between queries; and (c)
since visual analysis must happen in real-time, \SeeDB\ must tradeoff accuracy
of views for reduced latency. In Section \ref{sec:system_architecture}, we
describe how \SeeDB\ addresses these challenges.
 
Our contributions are as follows XXX:
\begin{denselist}
  \item We present a practical implementation of the \SeeDB\ system based on and
  by extending the original vision paper \cite{DBLP:conf/vldb/Parameswaran2013}.
  \item We discuss and evaluate various optimizations required to build such a
  system with realistic response rates. We also discuss how to make
  interactivity a part of the system.
  \item We show results of our user study demonstrating the utility of such a
  tool and the associated performance studies.
\end{denselist}


% \noindent There are several technical challenges that need to be addressed:
% 
% \begin{denselist}
% 
% \item For a given query, $n$, the total number of discriminating views, (even if
% we restrict ourselves to views that append a group-by and an aggregation) is
% likely to be very large to explore exhaustively and precisely. Generating each
% of $R_1(Q(D)),$  $\ldots,$ $R_n(Q(D))$, scoring them on utility, and then
% picking the best one is certainly not feasible for most databases. Thus, we need
% mechanisms to prune the space of views and compute their utility approximately.
% 
% \item Generating and scoring the discriminating views $R_i(Q(D))$ one-by-one may
% miss interesting optimization opportunities: First, we may share computation
% between discriminating views.  For example, the results of two views with
% different aggregates but the same group-by may be computed together in one
% query, followed by projecting out to reveal the two individual views.  Second,
% by evaluating the discriminating views in a deliberate order, we may be able to
% prune views with low utility (without evaluation) that are definitely not going
% to be recommended to the analyst.
% 
% \item Since visualizations tend to convey approximate information, e.g., a trend
% in a line plot may be more important than knowing the exact coordinates of each
% point, we can introduce approximations as part of \SeeDB.  Thus, the utility of
% a discriminating view may be computed approximately but efficiently, and the
% recommended discriminating views can be populated with approximate results,
% based on synopses of the base data or of the query result, that can be generated
% much more efficiently.
% 
% \end{denselist}
