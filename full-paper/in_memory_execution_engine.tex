\section{In Memory Execution Engine}
\label{sec:in_memory_execution_engine}

While the DBMS-backed execution engine discussed in the previous section allows
\SeeDB\ to be used with a variety of existing databases, optimization
opportunities are limited by virtue of being outside the database. In this
section, we describe our in-memory execution engine that offers the following
advantages over the DBMS-backed engine: (1) complete sharing of table scans
between views so that a single table scan sufficies to compute all views; (2)
sharing of utility estimates between views so that low utility views can be
pruned on the fly; (3) potential early stopping once the top views have been
identified.

\subsection{Basic Framework}
\label{subsec:basic_framework}
The basic in-memory solution works as follows: \SeeDB\ buffers the data file
from disk and processes the file one record at a time. Specifically, for each
record, \SeeDB\ determines whether the record satisfies the input query and then
updates the target and comparison views for all views stubs generated by the
View Generator. It also continually updates the utility values for each view as
more data is read in. Once the entire file has been scanned, \SeeDB\ picks the
views with the highest utility and returns them to the frontend.

As expected, this process avoids the multiple table scans that are performed by
the DBMS-backed execution engine. 
However, we can further improve performance if we can reduce the number of views
that are updated for each record (i.e. prune views on the fly) and stop early
once we have identified the top records. 
We introduce two classes of heuristics to perform these optimizations.
We discuss these next.

\subsection{Heuristics}
\label{subsec:heuristics}

The general idea that is used in both heuristics is to keep running estimates of
utility for each view under consideration, and perform pruning of low utility
views based on these estimates. 
As more records are scanned, the utility estimates become more accurate and we
can have higher confidence in our pruning decisions.

\subsubsection{Confidence Interval Based Pruning}
This pruning strategy is based on the application of sampling and confidence
intervals to utility estimates. 
For each view in the running, \SeeDB\ keeps a
running estimate of the mean and standard deviation of the view utility. 
These
together are used to generate 95\% confidence intervals for the utility at every
step. 
We support both the Hoeffding inequality based confidence interval calculation
as well as the more familiar normal distribution based calculation.

<<Insert formulas>>

As with similar top-k techniques, we use the following rule
to prune views: {\it if the upperbound (based on the confidence interval) on the
utility of view $v$ is lower than the lowest lowerbound of any other view $u$
in having the highest k utilities, then with 95\% probability, we can say
that view $v$ will never be in the top-k views}.

The algorithm (see Box \ref{}) proceeds as follows. 
Processing of the entire
file is divided into a number of phases (the number of phases $P$ is tunable
and is set to $>$$k$). 
The set of views in the running (i.e. views likely to be in the top-k) is set to
the view stubs generated by the View Generator.
During each phase, records are
processed and the utility mean and standard deviation are updated
continuously for the views still in the running.
At the end of a phase, confidence intervals (CIs) for utility are calculated for
every view in the running. 
The CIs are sorted in order of their upperbounds and the lowest lowerbound is
calculated for the top-k CIs. 
Views whose utility upperbound is lower than this lowerbound are discarded.
Once the whole file has been processed, all views in the running have
exact values for utility and \SeeDB\ picks the k views with the highest utility.
Note that if at any point the number of views in the running is equal to k, and
approximate view visualizations are acceptable, \SeeDB\ can stop processing the
file.

There are a few assumptions that are made here that we must account for: (a)
confidence interval formulas assume that each sample that is taken comes from
the same underlying probability distribution, and (b) the number of records in
each sample is the same. However, as we process increasing number of records,
the number of records in each sample changes and, therefore, we draw samples
from different distributions.
For example, when we have processed n=10K records, we are drawing from the
distribution in Figure \ref{} while for n=1M, we are drawing from the
distribution in Figure \ref{}).
Ideally, we would like to sample from the distribution in Figure \ref{}, but
since that is not possible, we are trying to approximate that distribution by
increasingly accurate distributions as in Figures \ref{}, \ref{}, and \ref{}.

To correct for the fact that our samples are of varying sizes, we apply the
following correction: \mpv{more}.

\subsubsection{Multi-Armed Bandit Heuristic}
\label{subsubsec:multi_armed_bandit}
The second set of heuristics we explore are derived by casting the problem of
finding the top-k views as an instance of the multi-armed bandit (MAB) problem. 
Traditionally, a MAB problem is where a gambler is faced with several slot
machines (``one armed bandits'') and the gambler must decide which machine to
play at each turn in order to maximize total reward. 
In our case, each view can be though of as a one-armed bandit and \SeeDB\ must
identify which bandits (or views) to evaluate to find the views with highest
reward (i.e. utility).
Recently, heuristics have been proposed for this top-k variation of the problem
(\cite{}). 
Specifically, given a set of bandits the goal of this MAB variation is to find
the $k$ arms with the highest mean.
We adapt the algorithm from \cite{} to identify the top-k arms.
Our technique works as follow (see Algorithm \ref{}).
Processing of the entire file is divided into a $k$ - 1 phases. 
The set of views in the running (i.e. views likely to be in the top-k) is set to
the view stubs generated by the View Generator.
During each phase, records are
processed and the utility mean is continuously updated for views still in
the running.
At the end of the phase, views are ranked in order of their means and
two special differences are calculated: $\Delta_1$ is the difference between the
highest mean and the $k+1$st highest mean, and $\Delta_n$ is the difference
between the lowest mean and the $k$th highest mean.
If $\Delta_1$ is greater than $\Delta_n$, the view with the highest mean is
{\it accepted} (i.e. it will no longer be used in pruning computations). On the
otherhand, if $\Delta_n$ is higher, the view with the lowest mean is discarded
from the set of views in the running.
Note again that if the number of views in the running is equal to $k$, it is
possible to stop early and provide the user an approximate set of views.

As before, we note that MAB formulations expect that each sample is drawn for
the same distribution and therefore we must make corrections accordingly. 
For the MAB heuristic, we simply discard utility values obtained in previous
phases, i.e. at the start of each phase calculations of the mean start anew.
This enables us to discard less accurate results (from previous phases) and
better approximate sampling from the same distribution.

In the next section, we perform an extensive experimental evaluation of all our
techniques on several real world and synthetic datasets.
