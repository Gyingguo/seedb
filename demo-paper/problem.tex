\section{Problem Statement}
\label{sec:problem_statement}

Given a database $D$ and a query $Q$, \SeeDB\ finds and visualizes the most
interesting aspects of $Q$ with respect to the underlying dataset. For this
purpose, \SeeDB\ considers various ``views'' of the query, where a view can be
some way of slicing or aggregating the data so that it may be visualized in
terms of histograms, time series charts etc. Currently \SeeDB\ limits views to
those that can be generated by adding one aggregate and one group-by clause
(group-by can have one or more attributes). Given an input query $Q$, a view
$Q_V$ may be described as $V(Q, A_i, (G_k\ldots Gl))$ where $Q$ is the original
query, $A_i$ is the aggregate attribute and $(G_k\ldots G_l)$ are attributes in
the group-by clause. The result of a view can be thought of as a two-column
table (e.g. sum of expenses for all patients, grouped by doctor and hospital)
that can be normalized to a probability distribution (e.g.
fraction of state's medical expenses per doctor and hospital). We denote the
distribution generated by $Q_V$ as $Pr(Q_V)$.

Since a large number of views are possible, \SeeDB\ must identify the iteresting
ones. To decide if a view is interesting, \SeeDB\ runs an equivalent view query
on the entire underlying dataset, i.e. $V(D, A_i, (G_k\ldots G_l))$, and obtains
a distribution for this view too. Let's call this distribution $Pr(D_V)$. We
posit that the utility or interesting-ness of a view depends on two things: (1)
how much $Pr(Q_V)$ deviates from $Pr(D_V)$ and (2) how complex is the view
$Q_V$. We prefer views with high deviation and low complexity. In the demo
system, \SeeDB\ measures view complexity simply as the number of attributes
added to the query. For measuring deviation among the query and dataset
distributions, \SeeDB\ can use a variety of existing metrics, a few of which are
discussed below. A user is able to select any of these metrics during his/her
analysis.  

\begin{itemize}
  \item {\bf Earth Movers Distance}: Commonly used to measure differences
  between color histograms from images, EMD is a popular metric for comparing
  discrete distributions.
  \item {\bf Euclidean Distance}: The L2 norm or Euclidean distance considers
  the two distributions are points in a high dimensional space and measures the
  distance between them.
  \item {\bf Kullback-Leibler Divergence}: K-L divergence measures the
  information lost when one probability distribution is used to approximate
  another.
  \item {\bf Jenson-Shannon Distance}: Based on the K-L divergence, this
  distance measures the similarity between two probability distributions.
\end{itemize}


Using the above notion of utility, \SeeDB\ produces the top-k views having the
high utility. Thus, formally, given database $D$, query $Q$ and positive integer
$k$, \SeeDB\ finds $k$ views of $Q$ that have the largest utility. 

%Trend in the subset of the data that deviates from the corresponding trend in
%the overall data.