\section{Problem Statement}
\label{sec:problem_statement}

In this demo paper, we describe our prototype of \SeeDB, a system to
automatically identify and visually highlight interesting aspects of a dataset.
Given a database $D$ and a query $Q$, \SeeDB\ finds and visualizes the most
interesting aspects of $Q$ with respect to the underlying dataset. For this
purpose, \SeeDB\ considers various ``views'' of the query, where a view can be
some way of slicing or aggregating the data so that it may be visualized in
terms of histograms, time series charts etc. Currently \SeeDB\ limits views to
those that can be generated by adding one aggregate and one group-by clause.
Given an input query $Q$, a view $Q_V$ may be described as $V(Q, A_i, G_j)$
where $Q$ is the original query, $A_i$ is the aggregate attribute and $G_j$ is
the group-by attribute. It is straightforward to extend the technique to
group-by clauses with multiple attributes. However, for ease of exposition and
visualization, we limit the number of attributes in the group-by clause to one.
The result of a view can be thought of as a two-column table (e.g. sum of
expenses for all patients, grouped by hospital) that can be
normalized to a probability distribution (e.g. fraction of state's medical
expenses per hospital). We denote the distribution generated by $Q_V$
as $Pr(Q_V)$.

To determine if a view for query $Q$ is interesting, in addition to executing
view query $Q_V$, \SeeDB\ executes an equivalent view query on the entire
underlying dataset, i.e.$V(D, A_i, G_j)$, to obtain two distributions:
distribution for the query, $Pr(Q_V)$ and distribution for the entire
dataset, $Pr(D_V)$. We measure the utility or interesting-ness of a view based on how
much $Pr(Q_V)$ deviates from $Pr(D_V)$; higher the deviation, higher the
utility.To measure deviation between distributions, \SeeDB\ can use a variety of
existing metrics, a few of which are discussed below. A user is able to select
any of these metrics during his/her analysis.

\begin{itemize}
  \item {\bf Earth Movers Distance}: Commonly used to measure differences
  between color histograms from images, EMD is a popular metric for comparing
  discrete distributions.
  \item {\bf Euclidean Distance}: The L2 norm or Euclidean distance considers
  the two distributions are points in a high dimensional space and measures the
  distance between them.
  \item {\bf Kullback-Leibler Divergence}: K-L divergence measures the
  information lost when one probability distribution is used to approximate
  another.
  \item {\bf Jenson-Shannon Distance}: Based on the K-L divergence, this
  distance measures the similarity between two probability distributions.
\end{itemize}


Using the above notion of utility, \SeeDB\ produces the top-k views having the
high utility. Thus, formally, given database $D$, query $Q$ and positive integer
$k$, \SeeDB\ finds $k$ views of $Q$ that have the largest utility. 

%Trend in the subset of the data that deviates from the corresponding trend in
%the overall data.