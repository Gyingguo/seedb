\section{Introduction}
\label{sec:intro}
Data scientists analyze large amounts of data to find trends and novel insights.
Given the large scale of data and relative ease of visual analysis, analysts
often use visualizations to identify trends of interest. The general workflow,
which is usually ad-hoc, can be represented as follows:

\begin{enumerate}
  \item Step 1: Analyst selects a subset of the data they are interested in; for
  instance, the analysts may select patients that had expenses more than twice
  the standard deviation of all patients, or a set of products of a certain kind.
  \item Step 2: The analyst processes and visualizes the data in various ways,
  e.g. via binning, aggregates, group-bys and then building
  visualizations such as scatterplots, histograms, pie charts etc.
  \item Step 3: The analyst studies all the views and identifies the
  {\it interesting} ones. He/she then drills down into these views or further
  explores the properties that show unusual trends. This can be done using sophisticated
  statistics or machine learning.
\end{enumerate}

The above process, while crucial in data analysis, is tedious and time-consuming
because the analyst must generate and study a large number of view of a
dataset. 

\subsection{Motivating Example}
\label{motivation_example}

Consider a medical researcher studying the cost of care for cancer patients. Her
research involves the analysis of a set of 1M electronic medical records (EMRs).
To analyze this data, the researcher identifies patients that cost
significantly more than the average: specifically, she selects patients whose
cost of care is greater than the average cost by two standard deviations. In
terms of SQL, she runs the following query: \\

\noindent Q = $SELECT * FROM Patients where total_cost - AVG(total_cost) >
2 * STDDEV (total_cost);$
<< fix this query >>

Once she has identified these patients, she must study various aspects of their
care to determine the reason why the patients have large cost of care. For
instance, she may study length of treatment, survival rate, severity of disease
etc. For each of these parameters, she is interested in determining whether the
group of patients with high cost of care are different from those with lower
cost of care. Since a large number of views of the data are possible, performing
these calculations repeatedly and visualizing the results is tedious and
time-consuming. Moreover, since the researcher may not be able to explore all
potential features, she may miss some interesting trends.

\SeeDB\ partially automates the process by computing and visualizing various
views of the data and highlighting the interesting views. Moreover, \SeeDB\ is
based on the principle that {\it what makes a view interesting is deviations
from expected behavior}. Therefore, given the subset of data selected using a
relational query, what makes the data interesting are trends or properties that
are different for this subset compared to the underlying complete dataset.

Ultimately, the analyst must decide if deviations in the data are interesting
but we can use deviation as a means to eliminate the laborious process
of stepping through all possible views of a dataset. 

\subsection{State-of-the-Art Approaches}
\label{related_work}

Over the past few years, there has been a significant
effort from the visualization community to provide interactive tools
for data analysts. In particular, tools such as ShowMe, Polaris, and
Tableau~\cite{DBLP:journals/cacm/StolteTH08,
  DBLP:journals/tvcg/MackinlayHS07} provide a canvas for data analysts
to manipulate and view data, tools such as
Wrangler~\cite{DBLP:conf/chi/KandelPHH11} allow data analysts to
transform and clean data, and tools such as
Profiler~\cite{DBLP:conf/avi/KandelPPHH12} allow users to visualize
simple anomalies in data.  However, unlike \SeeDB, these tools have
little automation; in effect, it is up to the analyst to generate a
two-column result (like the result of the discriminating view)
to be visualized. Other related areas of work include OLAP and database
visualization tools. There has been some work on browsing data cubes, allowing
analysts to variously find ``explanations'' for why two cube values were
different, to find which neighboring cubes have similar properties to the cube
under consideration, or get suggestions on what unexplored data cubes should be
looked at next~\cite{DBLP:conf/vldb/Sarawagi99, DBLP:conf/vldb/SatheS01,
DBLP:conf/vldb/Sarawagi00}.

Fusion tables~\cite{DBLP:conf/sigmod/GonzalezHJLMSSG10} allows users to create
visualizations layered on top of web databases; they do not consider the problem
of automatic visualization generation.
Devise~\cite{DBLP:conf/sigmod/LivnyRBCDLMW97} translated user-manipulated
visualizations into database queries.


% \noindent There are several technical challenges that need to be addressed:
% 
% \begin{denselist}
% 
% \item For a given query, $n$, the total number of discriminating views, (even if
% we restrict ourselves to views that append a group-by and an aggregation) is
% likely to be very large to explore exhaustively and precisely. Generating each
% of $R_1(Q(D)),$  $\ldots,$ $R_n(Q(D))$, scoring them on utility, and then
% picking the best one is certainly not feasible for most databases. Thus, we need
% mechanisms to prune the space of views and compute their utility approximately.
% 
% \item Generating and scoring the discriminating views $R_i(Q(D))$ one-by-one may
% miss interesting optimization opportunities: First, we may share computation
% between discriminating views.  For example, the results of two views with
% different aggregates but the same group-by may be computed together in one
% query, followed by projecting out to reveal the two individual views.  Second,
% by evaluating the discriminating views in a deliberate order, we may be able to
% prune views with low utility (without evaluation) that are definitely not going
% to be recommended to the analyst.
% 
% \item Since visualizations tend to convey approximate information, e.g., a trend
% in a line plot may be more important than knowing the exact coordinates of each
% point, we can introduce approximations as part of \SeeDB.  Thus, the utility of
% a discriminating view may be computed approximately but efficiently, and the
% recommended discriminating views can be populated with approximate results,
% based on synopses of the base data or of the query result, that can be generated
% much more efficiently.
% 
% \end{denselist}

In this demo paper, we describe our prototype of \SeeDB, a system to
automatically identify and visually highlight interesting aspects of a dataset. 

