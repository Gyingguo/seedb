\section{Introduction}
\label{sec:intro}
Data scientists analyze large amounts of data to find interesting trends and
gain novel insights. Given the large scale of data and relative ease of visual
analysis, analysts often use visualizations as a means to identify properties of
interest. The general analysis workflow is usually ad-hoc and can be represented
as follows: First, the analyst selects a subset of the data they are interested
in; for instance, in a medical use-case (Section
\ref{subsec:motivation_example}), the analysts may select patients that had
expenses more than twice the standard deviation of all patients, or a set of
products of a certain kind. Next, the analyst processes the data in various
ways, e.g. via binning, aggregates, group-bys, to generate various ``views''of
the data and builds visualizations such as scatterplots, histograms, pie charts
etc. Finally, the analyst studies all the views and identifies the {\it
interesting} ones. He/she then drills down into these views or further explores
the properties that show unusual trends. This can be done using sophisticated
statistics or machine learning.

The above process, while crucial in data analysis, is tedious and time-consuming
because the analyst must generate a large number of view of a dataset and
determine if each one is interesting or not. We believe that we can automate
several of the labor-intensive parts of this process by automatically generating
different views of the data and surfacing the most interesting ones to the user.

\subsection{Motivating Example}
\label{subsec:motivation_example}

Consider a medical researcher studying the cost of care for cancer patients. Her
research involves the analysis of a set of 1M electronic medical records (EMRs).
To analyze this data, the researcher identifies patients that cost
significantly more than the average: specifically, she selects patients whose
cost of care is greater than the average cost by two standard deviations. In
terms of SQL, she runs the following query: \\

\noindent 
\begin{small}
\begin{verbatim}
Q = SELECT * FROM Patients where total_cost - 
(SELECT AVG(total_cost) from Patients) as avg_cost
> 2 * (SELECT STDDEV(total_cost) from Patients);
\end{verbatim}
\end{small}


Once she has identified these patients, she must study various aspects of their
care to determine the reason why the patients have large cost of care. For
instance, she may study length of treatment, survival rate, severity of disease
etc. For each of these parameters, she is interested in determining whether the
group of patients with high cost of care are different from the overall group of
patients. Since a large number of views of the data are possible, performing
these calculations repeatedly and visualizing the results is tedious and
time-consuming. Moreover, since the researcher may not be able to explore all
potential features, she may miss some interesting trends.

In this demo, we propose a system called \SeeDB\ that partially automates the
data analysis process. Given a query posed by the researcher, \SeeDB\ can
compute a large number of views based on that query, determine the
``interesting''-ness or utility of each of the view and show to the user only
those views that it deems most interesting. The researcher can then further
explore these views. 

Since \SeeDB\ must rank views based on utility, accurately measuring utility is
cruicial. \SeeDB\ is based on the principle that it is the {\it deviations from
expected behavior that make a view interesting}. For instance, in the above
example, the researcher would be interested in the fact that it is patients
insured by a particular company that consistently have the highest expenses or
that survival for this group of patients doesn't improve with strong drug
regimens although it does for the full dataset of patients. Specifically, given
a query, interesting trends are those that differ significantly between the
query and the underlying dataset. Therefore, we must prefer views that show
these divergent trends.

Ultimately, the researcher must decide if deviations in the data are
interesting, but in \SeeDB\ we can use deviation as a means to eliminate the
laborious process of stepping through all possible views of a dataset.
In the process of automatically producing an interesting set of views for any
query, \SeeDB\ must address a few challenges: (a) the size of
the space of potential views increases exponentially with the number of
attributes in a table, as a result, \SeeDB\ must intelligently explore this
space; (b) computing each view and its utility independently is expensive and
wasteful, and hence \SeeDB\ must share computation between queries; and (c)
since visual analysis must happen in real-time, \SeeDB\ must tradeoff accuracy
of views for reduced latency. In Section \ref{sec:system_architecture}, we
describe how \SeeDB\ addresses these challenges.

\subsection{State-of-the-Art Approaches}
\label{related_work}

Over the past few years, there has been a significant
effort from the visualization community to provide interactive tools
for data analysts. In particular, tools such as ShowMe, Polaris, and
Tableau~\cite{DBLP:journals/cacm/StolteTH08,
  DBLP:journals/tvcg/MackinlayHS07} provide a canvas for data analysts
to manipulate and view data, tools such as
Wrangler~\cite{DBLP:conf/chi/KandelPHH11} allow data analysts to
transform and clean data, and tools such as
Profiler~\cite{DBLP:conf/avi/KandelPPHH12} allow users to visualize
simple anomalies in data.  However, unlike \SeeDB, these tools have
little automation; in effect, it is up to the analyst to generate a
two-column result (like the result of the discriminating view)
to be visualized. Other related areas of work include OLAP and database
visualization tools. There has been some work on browsing data cubes, allowing
analysts to variously find ``explanations'' for why two cube values were
different, to find which neighboring cubes have similar properties to the cube
under consideration, or get suggestions on what unexplored data cubes should be
looked at next~\cite{DBLP:conf/vldb/Sarawagi99, DBLP:conf/vldb/SatheS01,
DBLP:conf/vldb/Sarawagi00}.

Fusion tables~\cite{DBLP:conf/sigmod/GonzalezHJLMSSG10} allows users to create
visualizations layered on top of web databases; they do not consider the problem
of automatic visualization generation.
Devise~\cite{DBLP:conf/sigmod/LivnyRBCDLMW97} translated user-manipulated
visualizations into database queries.


% \noindent There are several technical challenges that need to be addressed:
% 
% \begin{denselist}
% 
% \item For a given query, $n$, the total number of discriminating views, (even if
% we restrict ourselves to views that append a group-by and an aggregation) is
% likely to be very large to explore exhaustively and precisely. Generating each
% of $R_1(Q(D)),$  $\ldots,$ $R_n(Q(D))$, scoring them on utility, and then
% picking the best one is certainly not feasible for most databases. Thus, we need
% mechanisms to prune the space of views and compute their utility approximately.
% 
% \item Generating and scoring the discriminating views $R_i(Q(D))$ one-by-one may
% miss interesting optimization opportunities: First, we may share computation
% between discriminating views.  For example, the results of two views with
% different aggregates but the same group-by may be computed together in one
% query, followed by projecting out to reveal the two individual views.  Second,
% by evaluating the discriminating views in a deliberate order, we may be able to
% prune views with low utility (without evaluation) that are definitely not going
% to be recommended to the analyst.
% 
% \item Since visualizations tend to convey approximate information, e.g., a trend
% in a line plot may be more important than knowing the exact coordinates of each
% point, we can introduce approximations as part of \SeeDB.  Thus, the utility of
% a discriminating view may be computed approximately but efficiently, and the
% recommended discriminating views can be populated with approximate results,
% based on synopses of the base data or of the query result, that can be generated
% much more efficiently.
% 
% \end{denselist}
